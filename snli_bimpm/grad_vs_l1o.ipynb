{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from bimpm import BIMPM\n",
    "from dataset import SNLI\n",
    "from util import prepare_output_dir\n",
    "from args import conf, rawr_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "\n",
    "    def __init__(self, premise=None, hypothesis=None, label=None):\n",
    "        self.premise = premise\n",
    "        self.hypothesis = hypothesis\n",
    "        self.label = label\n",
    "        \n",
    "def to_text(x, vocab):\n",
    "    if isinstance(x, Variable):\n",
    "        x = x.data\n",
    "    if isinstance(x, torch.cuda.LongTensor):\n",
    "        x = x.cpu()\n",
    "    if isinstance(x, torch.LongTensor):\n",
    "        x = x.numpy().tolist()\n",
    "    return ' '.join(vocab[w] for w in x if w != 1)\n",
    "\n",
    "def real_length(x):\n",
    "    # length of vector without padding\n",
    "    if isinstance(x, Variable):\n",
    "        return sum(x.data != 1)\n",
    "    else:\n",
    "        return sum(x != 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onehot_grad(model, batch, p_not_h=False):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    extracted_grads = {}\n",
    "\n",
    "    def hook(grad):\n",
    "        extracted_grads['embed'] = grad\n",
    "\n",
    "    batch_size, length = batch.premise.shape if p_not_h else batch.hypothesis.shape\n",
    "    model.train()  # turn on train mode here but we skip dropout with no_dropout in the foward call\n",
    "    output = model(batch.premise, batch.hypothesis, embed_grad_hook=hook, p_not_h=p_not_h, no_dropout=True)\n",
    "    label = torch.max(output, 1)[1]\n",
    "    loss = criterion(output, label)\n",
    "    loss.backward()\n",
    "    \n",
    "    embed = model.word_emb(batch.premise if p_not_h else batch.hypothesis)\n",
    "    onehot_grad = embed.view(-1) * extracted_grads['embed'].contiguous().view(-1)\n",
    "    onehot_grad = onehot_grad.view(batch_size, length, -1).sum(-1)\n",
    "    return onehot_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onehot_grad_wrong(model, batch):\n",
    "    # don't do this, this approach is wrong\n",
    "    # when you have duplicate tokens in a sentence, the gradient is aggregated\n",
    "    # say one token appeared twice, then the gradient is double of the correct gradient\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    batch_size, length = batch.hypothesis.shape\n",
    "    output = model(batch.premise, batch.hypothesis, no_dropout=True)\n",
    "    label = torch.max(output, 1)[1]\n",
    "    loss = criterion(output, label)\n",
    "    embed = model.word_emb(batch.hypothesis)\n",
    "    grad_auto = torch.autograd.grad(loss, model.word_emb.weight, create_graph=True)[0]\n",
    "    onehot_grad = embed.view(-1) * grad_auto[batch.hypothesis].view(-1)\n",
    "    onehot_grad = onehot_grad.view(batch_size, length, -1).sum(-1)\n",
    "    return onehot_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad_rank(batch):\n",
    "    one_hot_grad = get_onehot_grad(model, batch).detach().cpu().numpy()\n",
    "    real_lengths = [real_length(x) for x in batch.hypothesis]    \n",
    "    # sort by gradient of CrossEntropyLoss w.r.t. embedding * embedding\n",
    "    # large gradient means large increase in loss when embedding is increased by epsilon\n",
    "    # large gradient means large decrease in loss when embedding is decreased by epsilon\n",
    "    # large gradient approx large decrease in loss when word is removed\n",
    "    # large gradient approx small decrease in confidence when word is removed\n",
    "    # large gradient means word is unimportant\n",
    "    # first word is the least important\n",
    "    rank = [np.argsort(-x[:l]).tolist() for x, l in zip(one_hot_grad, real_lengths)]\n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_l1o_rank(batch):\n",
    "    # everything runs in eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    # original prediction and confidence\n",
    "    output = F.softmax(model(batch.premise, batch.hypothesis), 1)\n",
    "    target_scores, target = torch.max(output, 1)\n",
    "    \n",
    "    # decrease in confidence on the original prediction\n",
    "    losses = []\n",
    "    x = batch.hypothesis\n",
    "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "    # enumerate through words to be removed\n",
    "    for i in range(x.shape[1]):\n",
    "        # construct new hypothesis with ith word removed\n",
    "        hypothesis_parts = []\n",
    "        if i > 0:\n",
    "            hypothesis_parts.append(x[:, :i])\n",
    "        if i < x.shape[1] - 1:\n",
    "            hypothesis_parts.append(x[:, i+1:])\n",
    "        hypothesis = torch.cat(hypothesis_parts, axis=1)\n",
    "        \n",
    "        output = F.softmax(model(batch.premise, hypothesis), 1)\n",
    "        # new confidence on the original prediction\n",
    "        new_scores = output[np.arange(x.shape[0]), target]\n",
    "        # decrease in confidence on the original prediction\n",
    "        # small decrease means ith word is unimportant\n",
    "        losses.append((target_scores - new_scores).detach().cpu().numpy())\n",
    "        \n",
    "    # sort the decrease in confidence by ascending order\n",
    "    # small decrease means unimportant\n",
    "    # the first word is the least important word\n",
    "    losses = np.stack(losses, axis=1)\n",
    "    real_lengths = [real_length(x) for x in batch.hypothesis]    \n",
    "    rank = [np.argsort(x[:l]).tolist() for x, l in zip(losses, real_lengths)]\n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from data/\n",
      "loading vector cache from .vector_cache/vocab.vectors.pt\n"
     ]
    }
   ],
   "source": [
    "data = SNLI(conf)\n",
    "conf.char_vocab_size = len(data.char_vocab)\n",
    "conf.word_vocab_size = len(data.TEXT.vocab)\n",
    "conf.class_size = len(data.LABEL.vocab)\n",
    "conf.max_word_len = data.max_word_len\n",
    "q_vocab = data.TEXT.vocab.itos\n",
    "a_vocab = data.LABEL.vocab.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BIMPM(conf, data)\n",
    "model.load_state_dict(torch.load('results/baseline.pt'))\n",
    "model.word_emb.weight.requires_grad = True\n",
    "model = model.to(conf.device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 308/308 [01:27<00:00,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exact match important 0.4882137776874619\n",
      "exact match unimportant 0.37502540134119083\n",
      "top 3 important 0.8038000406421459\n",
      "top 3 unimportant 0.6494614915667547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "em_important, em_unimportant = [], []\n",
    "top3_important, top3_unimportant = [], []\n",
    "for i, batch in enumerate(tqdm(data.dev_iter)):\n",
    "    # if i > 5:\n",
    "    #     break\n",
    "    rank1 = get_grad_rank(batch)\n",
    "    rank2 = get_l1o_rank(batch)\n",
    "    em_important += [a[-1] == b[-1] for a, b in zip(rank1, rank2)]\n",
    "    em_unimportant += [a[0] == b[0] for a, b in zip(rank1, rank2)]\n",
    "    top3_important += [a[-1] in b[-1:-4:-1] for a, b in zip(rank1, rank2)]\n",
    "    top3_unimportant += [a[0] in b[:3] for a, b in zip(rank1, rank2)]\n",
    "print('exact match important', np.mean(em_important))\n",
    "print('exact match unimportant', np.mean(em_unimportant))\n",
    "print('top 3 important', np.mean(top3_important))\n",
    "print('top 3 unimportant', np.mean(top3_unimportant))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
