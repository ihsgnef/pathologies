{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from bimpm import BIMPM\n",
    "from dataset import SNLI\n",
    "from util import prepare_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from args import conf, rawr_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 32 from SNLI]\n",
       "\t[.premise]:[torch.cuda.LongTensor of size 32x7 (GPU 0)]\n",
       "\t[.hypothesis]:[torch.cuda.LongTensor of size 32x7 (GPU 0)]\n",
       "\t[.label]:[torch.cuda.LongTensor of size 32 (GPU 0)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Batch:\n",
    "\n",
    "    def __init__(self, premise=None, hypothesis=None, label=None):\n",
    "        self.premise = premise\n",
    "        self.hypothesis = hypothesis\n",
    "        self.label = label\n",
    "        \n",
    "def to_text(x, vocab):\n",
    "    if isinstance(x, Variable):\n",
    "        x = x.data\n",
    "    if isinstance(x, torch.cuda.LongTensor):\n",
    "        x = x.cpu()\n",
    "    if isinstance(x, torch.LongTensor):\n",
    "        x = x.numpy().tolist()\n",
    "    return ' '.join(vocab[w] for w in x if w != 1)\n",
    "\n",
    "def real_length(x):\n",
    "    # length of vector without padding\n",
    "    if isinstance(x, Variable):\n",
    "        return sum(x.data != 1)\n",
    "    else:\n",
    "        return sum(x != 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onehot_grad(model, batch, p_not_h=False):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    extracted_grads = {}\n",
    "\n",
    "    def extract_grad_hook(name):\n",
    "        def hook(grad):\n",
    "            extracted_grads[name] = grad\n",
    "        return hook\n",
    "\n",
    "    if p_not_h:\n",
    "        batch_size, length = batch.premise.shape\n",
    "    else:\n",
    "        batch_size, length = batch.hypothesis.shape\n",
    "    model.train()\n",
    "    output = model(batch.premise, batch.hypothesis,\n",
    "                   embed_grad_hook=extract_grad_hook('embed'),\n",
    "                   p_not_h=p_not_h)\n",
    "    label = torch.max(output, 1)[1]\n",
    "    loss = criterion(output, label)\n",
    "    loss.backward()\n",
    "    embed_grad = extracted_grads['embed']\n",
    "    if p_not_h:\n",
    "        embed = model.word_emb(batch.premise)\n",
    "    else:\n",
    "        embed = model.word_emb(batch.hypothesis)\n",
    "    onehot_grad = embed.view(-1) * embed_grad.contiguous().view(-1)\n",
    "    onehot_grad = onehot_grad.view(batch_size, length, -1).sum(-1)\n",
    "    return onehot_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad_rank(batch):\n",
    "    one_hot_grad = get_onehot_grad(model, batch).detach().cpu().numpy()\n",
    "    real_lengths = [real_length(x) for x in batch.hypothesis]    \n",
    "    # sort by gradient of CrossEntropyLoss w.r.t. embedding * embedding\n",
    "    # large gradient means large increase in loss when embedding is increased by epsilon\n",
    "    # large gradient means large decrease in loss when embedding is decreased by epsilon\n",
    "    # large gradient approx large decrease in loss when word is removed\n",
    "    # large gradient approx small decrease in confidence when word is removed\n",
    "    # large gradient means word is unimportant\n",
    "    # first word is the least important\n",
    "    rank = [np.argsort(-x[:l]).tolist() for x, l in zip(one_hot_grad, real_lengths)]\n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_l1o_rank(batch):\n",
    "    model.eval()\n",
    "    \n",
    "    # original prediction and confidence\n",
    "    output = F.softmax(model(batch.premise, batch.hypothesis), 1)\n",
    "    target_scores, target = torch.max(output, 1)\n",
    "    \n",
    "    # decrease in confidence on the original prediction\n",
    "    losses = []\n",
    "    x = batch.hypothesis\n",
    "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "    # enumerate through words to be removed\n",
    "    for i in range(x.shape[1]):\n",
    "        # construct new hypothesis with ith word removed\n",
    "        hypothesis_parts = []\n",
    "        if i > 0:\n",
    "            hypothesis_parts.append(x[:, :i])\n",
    "        if i < x.shape[1] - 1:\n",
    "            hypothesis_parts.append(x[:, i+1:])\n",
    "        hypothesis = torch.cat(hypothesis_parts, axis=1)\n",
    "        \n",
    "        output = F.softmax(model(batch.premise, hypothesis), 1)\n",
    "        # new confidence on the original prediction\n",
    "        new_scores = output[np.arange(x.shape[0]), target]\n",
    "        # decrease in confidence on the original prediction\n",
    "        # small decrease means ith word is unimportant\n",
    "        losses.append((target_scores - new_scores).detach().cpu().numpy())\n",
    "        \n",
    "    # sort the decrease in confidence by ascending order\n",
    "    # small decrease means unimportant\n",
    "    # the first word is the least important word\n",
    "    losses = np.stack(losses, axis=1)\n",
    "    real_lengths = [real_length(x) for x in batch.hypothesis]    \n",
    "    rank = [np.argsort(x[:l]).tolist() for x, l in zip(losses, real_lengths)]\n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BIMPM(conf, data)\n",
    "model.load_state_dict(torch.load('results/baseline.pt'))\n",
    "model.word_emb.weight.requires_grad = True\n",
    "model = model.to(conf.device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from data/\n",
      "loading vector cache from .vector_cache/vocab.vectors.pt\n"
     ]
    }
   ],
   "source": [
    "data = SNLI(conf)\n",
    "conf.char_vocab_size = len(data.char_vocab)\n",
    "conf.word_vocab_size = len(data.TEXT.vocab)\n",
    "conf.class_size = len(data.LABEL.vocab)\n",
    "conf.max_word_len = data.max_word_len\n",
    "q_vocab = data.TEXT.vocab.itos\n",
    "a_vocab = data.LABEL.vocab.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 308/308 [01:28<00:00,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exact match important 0.47561471245681775\n",
      "exact match unimportant 0.37014834383255435\n",
      "top 3 important 0.7985165616744564\n",
      "top 3 unimportant 0.6394025604551921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "em_important, em_unimportant = [], []\n",
    "top3_important, top3_unimportant = [], []\n",
    "for i, batch in enumerate(tqdm(data.dev_iter)):\n",
    "    # if i > 5:\n",
    "    #     break\n",
    "    rank1 = get_grad_rank(batch)\n",
    "    rank2 = get_l1o_rank(batch)\n",
    "    em_important += [a[-1] == b[-1] for a, b in zip(rank1, rank2)]\n",
    "    em_unimportant += [a[0] == b[0] for a, b in zip(rank1, rank2)]\n",
    "    top3_important += [a[-1] in b[-1:-4:-1] for a, b in zip(rank1, rank2)]\n",
    "    top3_unimportant += [a[0] in b[:3] for a, b in zip(rank1, rank2)]\n",
    "print('exact match important', np.mean(em_important))\n",
    "print('exact match unimportant', np.mean(em_unimportant))\n",
    "print('top 3 important', np.mean(top3_important))\n",
    "print('top 3 unimportant', np.mean(top3_unimportant))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
